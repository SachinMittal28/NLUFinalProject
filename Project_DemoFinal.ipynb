{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from babi_loader import BabiDataset, pad_collate\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def interpret_indexed_tensor(var):\n",
    "    qa= dset.QA\n",
    "    if len(var.size()) == 3:\n",
    "        # var -> n x #sen x #token\n",
    "        for n, sentences in enumerate(var):\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                s = ' '.join([qa.IVOCAB[elem.data[0]] for elem in sentence if qa.IVOCAB[elem.data[0]] != '<EOS>' or qa.IVOCAB[elem.data[0]] !=  '<PAD>'] )\n",
    "                print(f'{n}th of batch, {i}th sentence, {s}')\n",
    "    elif len(var.size()) == 2:\n",
    "        # var -> n x #token\n",
    "        for n, sentence in enumerate(var):\n",
    "            s = ' '.join([qa.IVOCAB[elem.data[0]] for elem in sentence if qa.IVOCAB[elem.data[0]] != '<EOS>' or qa.IVOCAB[elem.data[0]] !=  '<PAD>'] )\n",
    "            print(f'{n}th of batch, {s}')\n",
    "    elif len(var.size()) == 1:\n",
    "        # var -> n (one token per batch)\n",
    "        for n, token in enumerate(var):\n",
    "            s = qa.IVOCAB[token.data[0]]\n",
    "            print(f'{n}th of batch, {s}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from babi_loader import BabiDataset, pad_collate\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def position_encoding(embedded_sentence):\n",
    "    '''\n",
    "    embedded_sentence.size() -> (#batch, #sentence, #token, #embedding)\n",
    "    l.size() -> (#sentence, #embedding)\n",
    "    output.size() -> (#batch, #sentence, #embedding)\n",
    "    '''\n",
    "    _, _, slen, elen = embedded_sentence.size()\n",
    "\n",
    "    l = [[(1 - s/(slen-1)) - (e/(elen-1)) * (1 - 2*s/(slen-1)) for e in range(elen)] for s in range(slen)]\n",
    "    l = torch.FloatTensor(l)\n",
    "    l = l.unsqueeze(0) # for #batch\n",
    "    l = l.unsqueeze(1) # for #sen\n",
    "    l = l.expand_as(embedded_sentence)\n",
    "    weighted = embedded_sentence * Variable(l)\n",
    "    return torch.sum(weighted, dim=2).squeeze(2) # sum with tokens\n",
    "\n",
    "class AttentionGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttentionGRUCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wr = nn.Linear(input_size, hidden_size)\n",
    "        init.xavier_normal(self.Wr.state_dict()['weight'])\n",
    "        self.Ur = nn.Linear(hidden_size, hidden_size)\n",
    "        init.xavier_normal(self.Ur.state_dict()['weight'])\n",
    "        self.W = nn.Linear(input_size, hidden_size)\n",
    "        init.xavier_normal(self.W.state_dict()['weight'])\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        init.xavier_normal(self.U.state_dict()['weight'])\n",
    "\n",
    "    def forward(self, fact, C, g):\n",
    "        '''\n",
    "        fact.size() -> (#batch, #hidden = #embedding)\n",
    "        c.size() -> (#hidden, ) -> (#batch, #hidden = #embedding)\n",
    "        r.size() -> (#batch, #hidden = #embedding)\n",
    "        h_tilda.size() -> (#batch, #hidden = #embedding)\n",
    "        g.size() -> (#batch, )\n",
    "        '''\n",
    "\n",
    "        r = F.sigmoid(self.Wr(fact) + self.Ur(C))\n",
    "        h_tilda = F.tanh(self.W(fact) + r * self.U(C))\n",
    "        g = g.unsqueeze(1).expand_as(h_tilda)\n",
    "        h = g * h_tilda + (1 - g) * C\n",
    "        return h\n",
    "\n",
    "class AttentionGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttentionGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.AGRUCell = AttentionGRUCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, facts, G):\n",
    "        '''\n",
    "        facts.size() -> (#batch, #sentence, #hidden = #embedding)\n",
    "        fact.size() -> (#batch, #hidden = #embedding)\n",
    "        G.size() -> (#batch, #sentence)\n",
    "        g.size() -> (#batch, )\n",
    "        C.size() -> (#batch, #hidden)\n",
    "        '''\n",
    "        batch_num, sen_num, embedding_size = facts.size()\n",
    "        C = Variable(torch.zeros(self.hidden_size))\n",
    "        for sid in range(sen_num):\n",
    "            fact = facts[:, sid, :]\n",
    "            g = G[:, sid]\n",
    "            if sid == 0:\n",
    "                C = C.unsqueeze(0).expand_as(fact)\n",
    "            C = self.AGRUCell(fact, C, g)\n",
    "        return C\n",
    "\n",
    "class EpisodicMemory(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(EpisodicMemory, self).__init__()\n",
    "        self.AGRU = AttentionGRU(hidden_size, hidden_size)\n",
    "        self.z1 = nn.Linear(4 * hidden_size, hidden_size)\n",
    "        self.z2 = nn.Linear(hidden_size, 1)\n",
    "        print(\"hi.....\")\n",
    "        self.next_mem = nn.Linear(3 * hidden_size, hidden_size)\n",
    "        init.xavier_normal(self.z1.state_dict()['weight'])\n",
    "        init.xavier_normal(self.z2.state_dict()['weight'])\n",
    "        init.xavier_normal(self.next_mem.state_dict()['weight'])\n",
    "\n",
    "    def make_interaction(self, facts, questions, prevM):\n",
    "        '''\n",
    "        facts.size() -> (#batch, #sentence, #hidden = #embedding)\n",
    "        questions.size() -> (#batch, 1, #hidden)\n",
    "        prevM.size() -> (#batch, #sentence = 1, #hidden = #embedding)\n",
    "        z.size() -> (#batch, #sentence, 4 x #embedding)\n",
    "        G.size() -> (#batch, #sentence)\n",
    "        '''\n",
    "        batch_num, sen_num, embedding_size = facts.size()\n",
    "        questions = questions.expand_as(facts)\n",
    "        prevM = prevM.expand_as(facts)\n",
    "\n",
    "        z = torch.cat([\n",
    "            facts * questions,\n",
    "            facts * prevM,\n",
    "            torch.abs(facts - questions),\n",
    "            torch.abs(facts - prevM)\n",
    "        ], dim=2)\n",
    "\n",
    "        z = z.view(-1, 4 * embedding_size)\n",
    "\n",
    "        G = F.tanh(self.z1(z))\n",
    "        G = self.z2(G)\n",
    "        G = G.view(batch_num, -1)\n",
    "        G = F.softmax(G)\n",
    "\n",
    "        return G\n",
    "\n",
    "    def forward(self, facts, questions, prevM):\n",
    "        '''\n",
    "        facts.size() -> (#batch, #sentence, #hidden = #embedding)\n",
    "        questions.size() -> (#batch, #sentence = 1, #hidden)\n",
    "        prevM.size() -> (#batch, #sentence = 1, #hidden = #embedding)\n",
    "        G.size() -> (#batch, #sentence)\n",
    "        C.size() -> (#batch, #hidden)\n",
    "        concat.size() -> (#batch, 3 x #embedding)\n",
    "        '''\n",
    "        G = self.make_interaction(facts, questions, prevM)\n",
    "        value, index = torch.max(G, dim=1)\n",
    "        print(\"attentions= \",G)\n",
    "        \n",
    "        print(\"focus = \")\n",
    "        print((index[0]))\n",
    "        #print(\"focus = \",interpret_indexed_tensor(contexts))\n",
    "        #print(\"focus = \",(value[0]))\n",
    "\n",
    "        C = self.AGRU(facts, G)\n",
    "        concat = torch.cat([prevM.squeeze(1), C, questions.squeeze(1)], dim=1)\n",
    "        next_mem = F.relu(self.next_mem(concat))\n",
    "        next_mem = next_mem.unsqueeze(1)\n",
    "        return next_mem\n",
    "\n",
    "\n",
    "class QuestionModule(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(QuestionModule, self).__init__()\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, questions, word_embedding):\n",
    "        '''\n",
    "        questions.size() -> (#batch, #token)\n",
    "        word_embedding() -> (#batch, #token, #embedding)\n",
    "        gru() -> (1, #batch, #hidden)\n",
    "        '''\n",
    "        questions = word_embedding(questions)\n",
    "        _, questions = self.gru(questions)\n",
    "        questions = questions.transpose(0, 1)\n",
    "        return questions\n",
    "\n",
    "class InputModule(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(InputModule, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        for name, param in self.gru.state_dict().items():\n",
    "            if 'weight' in name: init.xavier_normal(param)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, contexts, word_embedding):\n",
    "        '''\n",
    "        contexts.size() -> (#batch, #sentence, #token)\n",
    "        word_embedding() -> (#batch, #sentence x #token, #embedding)\n",
    "        position_encoding() -> (#batch, #sentence, #embedding)\n",
    "        facts.size() -> (#batch, #sentence, #hidden = #embedding)\n",
    "        '''\n",
    "        batch_num, sen_num, token_num = contexts.size()\n",
    "\n",
    "        contexts = contexts.view(batch_num, -1)\n",
    "        contexts = word_embedding(contexts)\n",
    "\n",
    "        contexts = contexts.view(batch_num, sen_num, token_num, -1)\n",
    "        contexts = position_encoding(contexts)\n",
    "        contexts = self.dropout(contexts)\n",
    "\n",
    "        h0 = Variable(torch.zeros(2, batch_num, self.hidden_size))\n",
    "        facts, hdn = self.gru(contexts, h0)\n",
    "        facts = facts[:, :, :hidden_size] + facts[:, :, hidden_size:]\n",
    "        return facts\n",
    "\n",
    "class AnswerModule(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(AnswerModule, self).__init__()\n",
    "        self.z = nn.Linear(2 * hidden_size, vocab_size)\n",
    "        init.xavier_normal(self.z.state_dict()['weight'])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, M, questions):\n",
    "        M = self.dropout(M)\n",
    "        concat = torch.cat([M, questions], dim=2).squeeze(1)\n",
    "        z = self.z(concat)\n",
    "        return z\n",
    "\n",
    "class DMNPlus(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, num_hop=3, qa=None):\n",
    "        super(DMNPlus, self).__init__()\n",
    "        self.num_hop = num_hop\n",
    "        self.qa = qa\n",
    "        self.word_embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0, sparse=True)\n",
    "        init.uniform(self.word_embedding.state_dict()['weight'], a=-(3**0.5), b=3**0.5)\n",
    "        self.criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "\n",
    "        self.input_module = InputModule(vocab_size, hidden_size)\n",
    "        self.question_module = QuestionModule(vocab_size, hidden_size)\n",
    "        self.memory = EpisodicMemory(hidden_size)\n",
    "        self.answer_module = AnswerModule(vocab_size, hidden_size)\n",
    "\n",
    "    def forward(self, contexts, questions):\n",
    "        '''\n",
    "        contexts.size() -> (#batch, #sentence, #token) -> (#batch, #sentence, #hidden = #embedding)\n",
    "        questions.size() -> (#batch, #token) -> (#batch, 1, #hidden)\n",
    "        '''\n",
    "        facts = self.input_module(contexts, self.word_embedding)\n",
    "        questions = self.question_module(questions, self.word_embedding)\n",
    "        M = questions\n",
    "        for hop in range(self.num_hop):\n",
    "            M = self.memory(facts, questions, M)\n",
    "        preds = self.answer_module(M, questions)\n",
    "        return preds\n",
    "\n",
    "   \n",
    "    def get_loss(self, contexts, questions, targets):\n",
    "        output = self.forward(contexts, questions)\n",
    "        loss = self.criterion(output, targets)\n",
    "        reg_loss = 0\n",
    "        for param in self.parameters():\n",
    "            reg_loss += 0.001 * torch.sum(param * param)\n",
    "        preds = F.softmax(output)\n",
    "        _, pred_ids = torch.max(preds, dim=1)\n",
    "        s = self.qa.IVOCAB[pred_ids.data[0]]\n",
    "        print(\"\\npredicted answer - \", s)\n",
    "        corrects = (pred_ids.data == answers.data)\n",
    "        acc = torch.mean(corrects.float())\n",
    "        return loss + reg_loss, acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi.....\n",
      "contexts -> \n",
      "0th of batch, 0th sentence, mary got the milk . <EOS> <PAD> <PAD>\n",
      "0th of batch, 1th sentence, john moved to the bedroom . <EOS> <PAD>\n",
      "0th of batch, 2th sentence, daniel journeyed to the office . <EOS> <PAD>\n",
      "0th of batch, 3th sentence, john grabbed the apple there . <EOS> <PAD>\n",
      "0th of batch, 4th sentence, john got the football . <EOS> <PAD> <PAD>\n",
      "0th of batch, 5th sentence, john journeyed to the garden . <EOS> <PAD>\n",
      "0th of batch, 6th sentence, mary left the milk . <EOS> <PAD> <PAD>\n",
      "0th of batch, 7th sentence, john left the football . <EOS> <PAD> <PAD>\n",
      "0th of batch, 8th sentence, daniel moved to the garden . <EOS> <PAD>\n",
      "0th of batch, 9th sentence, daniel grabbed the football . <EOS> <PAD> <PAD>\n",
      "0th of batch, 10th sentence, mary moved to the hallway . <EOS> <PAD>\n",
      "0th of batch, 11th sentence, mary went to the kitchen . <EOS> <PAD>\n",
      "0th of batch, 12th sentence, john put down the apple there . <EOS>\n",
      "0th of batch, 13th sentence, john picked up the apple . <EOS> <PAD>\n",
      "0th of batch, 14th sentence, sandra moved to the hallway . <EOS> <PAD>\n",
      "0th of batch, 15th sentence, daniel left the football there . <EOS> <PAD>\n",
      "0th of batch, 16th sentence, daniel took the football . <EOS> <PAD> <PAD>\n",
      "0th of batch, 17th sentence, john travelled to the kitchen . <EOS> <PAD>\n",
      "0th of batch, 18th sentence, daniel dropped the football . <EOS> <PAD> <PAD>\n",
      "0th of batch, 19th sentence, john dropped the apple . <EOS> <PAD> <PAD>\n",
      "0th of batch, 20th sentence, john grabbed the apple . <EOS> <PAD> <PAD>\n",
      "0th of batch, 21th sentence, john went to the office . <EOS> <PAD>\n",
      "0th of batch, 22th sentence, sandra went back to the bedroom . <EOS>\n",
      "0th of batch, 23th sentence, sandra took the milk . <EOS> <PAD> <PAD>\n",
      "0th of batch, 24th sentence, john journeyed to the bathroom . <EOS> <PAD>\n",
      "0th of batch, 25th sentence, john travelled to the office . <EOS> <PAD>\n",
      "0th of batch, 26th sentence, sandra left the milk . <EOS> <PAD> <PAD>\n",
      "0th of batch, 27th sentence, mary went to the bedroom . <EOS> <PAD>\n",
      "0th of batch, 28th sentence, mary moved to the office . <EOS> <PAD>\n",
      "0th of batch, 29th sentence, john travelled to the hallway . <EOS> <PAD>\n",
      "0th of batch, 30th sentence, sandra moved to the garden . <EOS> <PAD>\n",
      "0th of batch, 31th sentence, mary moved to the kitchen . <EOS> <PAD>\n",
      "0th of batch, 32th sentence, daniel took the football . <EOS> <PAD> <PAD>\n",
      "0th of batch, 33th sentence, mary journeyed to the bedroom . <EOS> <PAD>\n",
      "0th of batch, 34th sentence, mary grabbed the milk there . <EOS> <PAD>\n",
      "0th of batch, 35th sentence, mary discarded the milk . <EOS> <PAD> <PAD>\n",
      "0th of batch, 36th sentence, john went to the garden . <EOS> <PAD>\n",
      "0th of batch, 37th sentence, john discarded the apple there . <EOS> <PAD>\n",
      "\n",
      "\n",
      " questions -> \n",
      "0th of batch, where was the apple before the bathroom <EOS>\n",
      "\n",
      "\n",
      " answers -> \n",
      "0th of batch, office\n",
      "attentions=  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0002  0.0005  0.0002  0.0002  0.0003  0.0093  0.0010  0.0026  0.0130  0.0011\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0160  0.0007  0.0002  0.0003  0.0003  0.0083  0.0015  0.0671  0.0002  0.0018\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.0021  0.3272  0.0017  0.0001  0.0096  0.0939  0.0005  0.0190  0.0013  0.1420\n",
      "\n",
      "Columns 30 to 37 \n",
      " 0.0011  0.0350  0.0062  0.0029  0.0042  0.0003  0.2271  0.0007\n",
      "[torch.FloatTensor of size 1x38]\n",
      "\n",
      "focus = \n",
      "Variable containing:\n",
      " 21\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "attentions=  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0002  0.0002  0.0001  0.0000  0.0001  0.0016  0.0003  0.0003  0.0003  0.0002\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0014  0.0001  0.0000  0.0001  0.0001  0.0007  0.0012  0.0449  0.0003  0.0014\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.0037  0.6016  0.0008  0.0000  0.0059  0.0616  0.0001  0.0027  0.0005  0.1531\n",
      "\n",
      "Columns 30 to 37 \n",
      " 0.0004  0.0025  0.0004  0.0005  0.0005  0.0000  0.1111  0.0009\n",
      "[torch.FloatTensor of size 1x38]\n",
      "\n",
      "focus = \n",
      "Variable containing:\n",
      " 21\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "attentions=  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0003  0.0006  0.0003  0.0001  0.0000  0.0012  0.0002  0.0002  0.0008  0.0003\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0025  0.0003  0.0000  0.0001  0.0001  0.0005  0.0017  0.0389  0.0003  0.0018\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.0020  0.5581  0.0010  0.0002  0.0202  0.0902  0.0002  0.0063  0.0013  0.1420\n",
      "\n",
      "Columns 30 to 37 \n",
      " 0.0013  0.0038  0.0012  0.0029  0.0012  0.0002  0.1158  0.0017\n",
      "[torch.FloatTensor of size 1x38]\n",
      "\n",
      "focus = \n",
      "Variable containing:\n",
      " 21\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      "predicted answer -  office\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachinmittal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:115: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/sachinmittal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:237: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    task_id = 3\n",
    "    dset = BabiDataset(task_id)\n",
    "    vocab_size = len(dset.QA.VOCAB)\n",
    "    hidden_size = 80\n",
    "    model = DMNPlus(hidden_size, vocab_size, num_hop=3, qa=dset.QA)\n",
    "    best_acc = 0\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    pretrained_dict_name = \"task\"+str(task_id)+\".pth\"\n",
    "    pretrained_dict = torch.load(pretrained_dict_name, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    #print(pretrained_dict.keys())\n",
    "    #print(\"\\n\\n\\n\")\n",
    "    #print(model_dict.keys())\n",
    "    dset.set_mode('test')\n",
    "    test_loader = DataLoader(\n",
    "        dset, batch_size=1, shuffle=False, collate_fn=pad_collate\n",
    "    )\n",
    "    test_acc = 0\n",
    "    cnt = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        contexts, questions, answers = data\n",
    "        batch_size = contexts.size()[0]\n",
    "        contexts = Variable(contexts.long())\n",
    "        questions = Variable(questions.long())\n",
    "        answers = Variable(answers)\n",
    "        \n",
    "        print(\"contexts -> \")\n",
    "        interpret_indexed_tensor(contexts)\n",
    "        print(\"\\n\\n questions -> \")\n",
    "\n",
    "        interpret_indexed_tensor(questions)\n",
    "        print(\"\\n\\n answers -> \")\n",
    "\n",
    "        interpret_indexed_tensor(answers)\n",
    "\n",
    "\n",
    "        \n",
    "        model.load_state_dict(pretrained_dict)\n",
    "        \n",
    "        _, acc = model.get_loss(contexts, questions, answers)\n",
    "        #print(acc)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['word_embedding.weight', 'input_module.gru.weight_ih_l0', 'input_module.gru.weight_hh_l0', 'input_module.gru.bias_ih_l0', 'input_module.gru.bias_hh_l0', 'input_module.gru.weight_ih_l0_reverse', 'input_module.gru.weight_hh_l0_reverse', 'input_module.gru.bias_ih_l0_reverse', 'input_module.gru.bias_hh_l0_reverse', 'question_module.gru.weight_ih_l0', 'question_module.gru.weight_hh_l0', 'question_module.gru.bias_ih_l0', 'question_module.gru.bias_hh_l0', 'memory.AGRU.AGRUCell.Wr.weight', 'memory.AGRU.AGRUCell.Wr.bias', 'memory.AGRU.AGRUCell.Ur.weight', 'memory.AGRU.AGRUCell.Ur.bias', 'memory.AGRU.AGRUCell.W.weight', 'memory.AGRU.AGRUCell.W.bias', 'memory.AGRU.AGRUCell.U.weight', 'memory.AGRU.AGRUCell.U.bias', 'memory.z1.weight', 'memory.z1.bias', 'memory.z2.weight', 'memory.z2.bias', 'memory.next_mem.weight', 'memory.next_mem.bias', 'answer_module.z.weight', 'answer_module.z.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
